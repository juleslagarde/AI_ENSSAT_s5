{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "jmjh290raIky"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Neural machine translation with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/nmt_with_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation. This is an advanced example that assumes some knowledge of sequence to sequence models.\n",
    "\n",
    "After training the model in this notebook, you will be able to input a Spanish sentence, such as *\"¿todavia estan en casa?\"*, and return the English translation: *\"are you still at home?\"*\n",
    "\n",
    "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n",
    "\n",
    "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
    "\n",
    "Note: This example takes approximately 10 minutes to run on a single P100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4989454288529581754\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4930941747\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3736969674184335931\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/wikipedia_tabs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = w.lower().strip()\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Zçàéêèù .,0-9'\\\"()\\[\\]{}=\\-;/:«»#%*ÉÈ!?]+\", \"\\x02\", w)\n",
    "\n",
    "  w = w.rstrip().strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '\\x00' + w + '\\x01'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opI2GzOt479E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000may i borrow this book?\u0001\n",
      "b'\\x00\\x02puedo tomar prestado este libro?\\x01'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples, max_length):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  lines = [l for l in lines if len(l)<=max_length]\n",
    "  print(\"%s lines with max length %s\"%(len(lines), max_length))\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTbSbBz55QtF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40650 lines with max length 80\n",
      "\u0000(p. joanne).\u0001\n",
      "\u0000p. joanne .\u0001\n"
     ]
    }
   ],
   "source": [
    "fr1, fr2 = create_dataset(path_to_file, None, 80)\n",
    "print(fr1[0])\n",
    "print(fr2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmMZQpdO60dt"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "  return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='', char_level=True)\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  inp_lang, targ_lang = create_dataset(path, num_examples, 80)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n",
    "\n",
    "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40650 lines with max length 80\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QILQkOs3jFG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXukARTDd7MT",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "7 ----> \u0000\n",
      "36 ----> :\n",
      "34 ----> k\n",
      "22 ----> b\n",
      "36 ----> :\n",
      "1 ---->  \n",
      "14 ----> u\n",
      "9 ----> n\n",
      "2 ----> e\n",
      "1 ---->  \n",
      "20 ----> g\n",
      "10 ----> r\n",
      "13 ----> o\n",
      "6 ----> s\n",
      "6 ----> s\n",
      "2 ----> e\n",
      "1 ---->  \n",
      "6 ----> s\n",
      "14 ----> u\n",
      "10 ----> r\n",
      "18 ----> p\n",
      "10 ----> r\n",
      "4 ----> i\n",
      "6 ----> s\n",
      "2 ----> e\n",
      "21 ----> ,\n",
      "1 ---->  \n",
      "13 ----> o\n",
      "14 ----> u\n",
      "4 ----> i\n",
      "52 ----> !\n",
      "8 ----> \u0001\n",
      "\n",
      "Target Language; index to word mapping\n",
      "12 ----> \u0000\n",
      "30 ----> k\n",
      "10 ----> .\n",
      "27 ----> b\n",
      "10 ----> .\n",
      "1 ---->  \n",
      "11 ----> u\n",
      "4 ----> n\n",
      "2 ----> e\n",
      "1 ---->  \n",
      "20 ----> g\n",
      "8 ----> r\n",
      "14 ----> o\n",
      "7 ----> s\n",
      "7 ----> s\n",
      "2 ----> e\n",
      "1 ---->  \n",
      "7 ----> s\n",
      "11 ----> u\n",
      "8 ----> r\n",
      "18 ----> p\n",
      "8 ----> r\n",
      "3 ----> i\n",
      "7 ----> s\n",
      "2 ----> e\n",
      "1 ---->  \n",
      "25 ----> ,\n",
      "1 ---->  \n",
      "14 ----> o\n",
      "11 ----> u\n",
      "3 ----> i\n",
      "1 ---->  \n",
      "37 ----> !\n",
      "13 ----> \u0001\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "tf.device('/GPU:0')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 44]), TensorShape([64, 67]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Write the encoder and decoder model\n",
    "\n",
    "Implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from [Luong's paper](https://arxiv.org/abs/1508.04025v5). \n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n",
    "\n",
    "Here are the equations that are implemented:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "This tutorial uses [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) for the encoder. Let's decide on notation before writing the simplified form:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* This merged vector is then given to the GRU\n",
    "\n",
    "The shapes of all the vectors at each step have been specified in the comments in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 44, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k534zTHiDjQU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 44, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 53)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['\\x00']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.7595\n",
      "Epoch 1 Batch 100 Loss 0.9615\n",
      "Epoch 1 Batch 200 Loss 0.8465\n",
      "Epoch 1 Batch 300 Loss 0.4386\n",
      "Epoch 1 Loss 0.7584\n",
      "Time taken for 1 epoch 378.9968886375427 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.1708\n",
      "Epoch 2 Batch 100 Loss 0.1450\n",
      "Epoch 2 Batch 200 Loss 0.1262\n",
      "Epoch 2 Batch 300 Loss 0.1062\n",
      "Epoch 2 Loss 0.1323\n",
      "Time taken for 1 epoch 328.8066358566284 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0823\n",
      "Epoch 3 Batch 100 Loss 0.0767\n",
      "Epoch 3 Batch 200 Loss 0.0435\n",
      "Epoch 3 Batch 300 Loss 0.0553\n",
      "Epoch 3 Loss 0.0730\n",
      "Time taken for 1 epoch 327.1417603492737 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0476\n",
      "Epoch 4 Batch 100 Loss 0.0635\n",
      "Epoch 4 Batch 200 Loss 0.0436\n",
      "Epoch 4 Batch 300 Loss 0.0349\n",
      "Epoch 4 Loss 0.0528\n",
      "Time taken for 1 epoch 326.2082414627075 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0448\n",
      "Epoch 5 Batch 100 Loss 0.0722\n",
      "Epoch 5 Batch 200 Loss 0.1603\n",
      "Epoch 5 Batch 300 Loss 0.0970\n",
      "Epoch 5 Loss 0.0716\n",
      "Time taken for 1 epoch 326.3894853591919 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1042\n",
      "Epoch 6 Batch 100 Loss 0.0415\n",
      "Epoch 6 Batch 200 Loss 0.0532\n",
      "Epoch 6 Batch 300 Loss 0.0363\n",
      "Epoch 6 Loss 0.0442\n",
      "Time taken for 1 epoch 326.862135887146 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0366\n",
      "Epoch 7 Batch 100 Loss 0.0290\n",
      "Epoch 7 Batch 200 Loss 0.0271\n",
      "Epoch 7 Batch 300 Loss 0.0330\n",
      "Epoch 7 Loss 0.0348\n",
      "Time taken for 1 epoch 325.9312016963959 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0245\n",
      "Epoch 8 Batch 100 Loss 0.0264\n",
      "Epoch 8 Batch 200 Loss 0.0365\n",
      "Epoch 8 Batch 300 Loss 0.0441\n",
      "Epoch 8 Loss 0.0359\n",
      "Time taken for 1 epoch 327.18205976486206 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0276\n",
      "Epoch 9 Batch 100 Loss 0.0279\n",
      "Epoch 9 Batch 200 Loss 0.0260\n",
      "Epoch 9 Batch 300 Loss 0.0275\n",
      "Epoch 9 Loss 0.0311\n",
      "Time taken for 1 epoch 325.4191401004791 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0253\n",
      "Epoch 10 Batch 100 Loss 0.0321\n",
      "Epoch 10 Batch 200 Loss 0.0267\n",
      "Epoch 10 Batch 300 Loss 0.0212\n",
      "Epoch 10 Loss 0.0266\n",
      "Time taken for 1 epoch 328.69506645202637 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0149\n",
      "Epoch 11 Batch 100 Loss 0.0142\n",
      "Epoch 11 Batch 200 Loss 0.0291\n",
      "Epoch 11 Batch 300 Loss 0.0374\n",
      "Epoch 11 Loss 0.0269\n",
      "Time taken for 1 epoch 325.34129428863525 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0190\n",
      "Epoch 12 Batch 100 Loss 0.0238\n",
      "Epoch 12 Batch 200 Loss 0.0123\n",
      "Epoch 12 Batch 300 Loss 0.0145\n",
      "Epoch 12 Loss 0.0277\n",
      "Time taken for 1 epoch 327.22422432899475 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0370\n",
      "Epoch 13 Batch 100 Loss 0.0578\n",
      "Epoch 13 Batch 200 Loss 0.0328\n",
      "Epoch 13 Batch 300 Loss 0.0211\n",
      "Epoch 13 Loss 0.0311\n",
      "Time taken for 1 epoch 325.41199946403503 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0895\n",
      "Epoch 14 Batch 100 Loss 0.4269\n",
      "Epoch 14 Batch 200 Loss 0.0653\n",
      "Epoch 14 Batch 300 Loss 0.0663\n",
      "Epoch 14 Loss 0.1668\n",
      "Time taken for 1 epoch 327.64427185058594 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0539\n",
      "Epoch 15 Batch 100 Loss 0.0433\n",
      "Epoch 15 Batch 200 Loss 0.0453\n",
      "Epoch 15 Batch 300 Loss 0.0291\n",
      "Epoch 15 Loss 0.0453\n",
      "Time taken for 1 epoch 325.3268394470215 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0374\n",
      "Epoch 16 Batch 100 Loss 0.0311\n",
      "Epoch 16 Batch 200 Loss 0.0450\n",
      "Epoch 16 Batch 300 Loss 0.0231\n",
      "Epoch 16 Loss 0.0398\n",
      "Time taken for 1 epoch 326.20861554145813 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0509\n",
      "Epoch 17 Batch 100 Loss 0.0365\n",
      "Epoch 17 Batch 200 Loss 0.0369\n",
      "Epoch 17 Batch 300 Loss 0.0477\n",
      "Epoch 17 Loss 0.0389\n",
      "Time taken for 1 epoch 325.4095480442047 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0288\n",
      "Epoch 18 Batch 100 Loss 0.0210\n",
      "Epoch 18 Batch 200 Loss 0.0221\n",
      "Epoch 18 Batch 300 Loss 0.0340\n",
      "Epoch 18 Loss 0.0261\n",
      "Time taken for 1 epoch 330.3488414287567 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0218\n",
      "Epoch 19 Batch 100 Loss 0.0297\n",
      "Epoch 19 Batch 200 Loss 0.0208\n",
      "Epoch 19 Batch 300 Loss 0.0170\n",
      "Epoch 19 Loss 0.0199\n",
      "Time taken for 1 epoch 325.2624433040619 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0133\n",
      "Epoch 20 Batch 100 Loss 0.0245\n",
      "Epoch 20 Batch 200 Loss 0.0326\n",
      "Epoch 20 Batch 300 Loss 0.0286\n",
      "Epoch 20 Loss 0.0221\n",
      "Time taken for 1 epoch 326.06702947616577 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0119\n",
      "Epoch 21 Batch 100 Loss 0.0143\n",
      "Epoch 21 Batch 200 Loss 0.0269\n",
      "Epoch 21 Batch 300 Loss 0.0326\n",
      "Epoch 21 Loss 0.0179\n",
      "Time taken for 1 epoch 325.27626633644104 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0130\n",
      "Epoch 22 Batch 100 Loss 0.0106\n",
      "Epoch 22 Batch 200 Loss 0.0175\n",
      "Epoch 22 Batch 300 Loss 0.0138\n",
      "Epoch 22 Loss 0.0153\n",
      "Time taken for 1 epoch 326.00660467147827 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0155\n",
      "Epoch 23 Batch 100 Loss 0.0135\n",
      "Epoch 23 Batch 200 Loss 0.0102\n",
      "Epoch 23 Batch 300 Loss 0.0783\n",
      "Epoch 23 Loss 0.0791\n",
      "Time taken for 1 epoch 325.2020971775055 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0745\n",
      "Epoch 24 Batch 100 Loss 0.1157\n",
      "Epoch 24 Batch 200 Loss 0.0534\n",
      "Epoch 24 Batch 300 Loss 0.0363\n",
      "Epoch 24 Loss 0.0836\n",
      "Time taken for 1 epoch 327.0212433338165 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0367\n",
      "Epoch 25 Batch 100 Loss 0.0350\n",
      "Epoch 25 Batch 200 Loss 0.0396\n",
      "Epoch 25 Batch 300 Loss 0.0253\n",
      "Epoch 25 Loss 0.0456\n",
      "Time taken for 1 epoch 325.11033034324646 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0297\n",
      "Epoch 26 Batch 100 Loss 0.0392\n",
      "Epoch 26 Batch 200 Loss 0.0531\n",
      "Epoch 26 Batch 300 Loss 0.0247\n",
      "Epoch 26 Loss 0.0348\n",
      "Time taken for 1 epoch 326.9774353504181 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0268\n",
      "Epoch 27 Batch 100 Loss 0.0203\n",
      "Epoch 27 Batch 200 Loss 0.0316\n",
      "Epoch 27 Batch 300 Loss 0.0346\n",
      "Epoch 27 Loss 0.0272\n",
      "Time taken for 1 epoch 325.2427523136139 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0264\n",
      "Epoch 28 Batch 100 Loss 0.0125\n",
      "Epoch 28 Batch 200 Loss 0.0181\n",
      "Epoch 28 Batch 300 Loss 0.0253\n",
      "Epoch 28 Loss 0.0199\n",
      "Time taken for 1 epoch 327.04688000679016 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0083\n",
      "Epoch 29 Batch 100 Loss 0.0134\n",
      "Epoch 29 Batch 200 Loss 0.0308\n",
      "Epoch 29 Batch 300 Loss 0.0178\n",
      "Epoch 29 Loss 0.0243\n",
      "Time taken for 1 epoch 325.2009689807892 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0181\n",
      "Epoch 30 Batch 100 Loss 0.0302\n",
      "Epoch 30 Batch 200 Loss 0.0139\n",
      "Epoch 30 Batch 300 Loss 0.0195\n",
      "Epoch 30 Loss 0.0209\n",
      "Time taken for 1 epoch 326.9706540107727 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0148\n",
      "Epoch 31 Batch 100 Loss 0.0190\n",
      "Epoch 31 Batch 200 Loss 0.0146\n",
      "Epoch 31 Batch 300 Loss 0.0105\n",
      "Epoch 31 Loss 0.0264\n",
      "Time taken for 1 epoch 325.1836302280426 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0274\n",
      "Epoch 32 Batch 100 Loss 0.0321\n",
      "Epoch 32 Batch 200 Loss 0.0356\n",
      "Epoch 32 Batch 300 Loss 0.0422\n",
      "Epoch 32 Loss 0.0430\n",
      "Time taken for 1 epoch 327.5618884563446 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0560\n",
      "Epoch 33 Batch 100 Loss 0.0233\n",
      "Epoch 33 Batch 200 Loss 0.0663\n",
      "Epoch 33 Batch 300 Loss 0.0303\n",
      "Epoch 33 Loss 0.0480\n",
      "Time taken for 1 epoch 325.21841835975647 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.2751\n",
      "Epoch 34 Batch 100 Loss 0.0799\n",
      "Epoch 34 Batch 200 Loss 0.0446\n",
      "Epoch 34 Batch 300 Loss 0.0325\n",
      "Epoch 34 Loss 0.0555\n",
      "Time taken for 1 epoch 326.93601393699646 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0258\n",
      "Epoch 35 Batch 100 Loss 0.1235\n",
      "Epoch 35 Batch 200 Loss 0.0874\n",
      "Epoch 35 Batch 300 Loss 0.1108\n",
      "Epoch 35 Loss 0.1413\n",
      "Time taken for 1 epoch 325.16213607788086 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0636\n",
      "Epoch 36 Batch 100 Loss 0.0657\n",
      "Epoch 36 Batch 200 Loss 0.0878\n",
      "Epoch 36 Batch 300 Loss 0.0469\n",
      "Epoch 36 Loss 0.0640\n",
      "Time taken for 1 epoch 327.569194316864 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0357\n",
      "Epoch 37 Batch 100 Loss 0.0379\n",
      "Epoch 37 Batch 200 Loss 0.0310\n",
      "Epoch 37 Batch 300 Loss 0.0569\n",
      "Epoch 37 Loss 0.0432\n",
      "Time taken for 1 epoch 325.2208933830261 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0463\n",
      "Epoch 38 Batch 100 Loss 0.0562\n",
      "Epoch 38 Batch 200 Loss 0.0310\n",
      "Epoch 38 Batch 300 Loss 0.0192\n",
      "Epoch 38 Loss 0.0472\n",
      "Time taken for 1 epoch 325.89505767822266 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0505\n",
      "Epoch 39 Batch 100 Loss 0.0429\n",
      "Epoch 39 Batch 200 Loss 0.0191\n",
      "Epoch 39 Batch 300 Loss 0.0342\n",
      "Epoch 39 Loss 0.0357\n",
      "Time taken for 1 epoch 325.17060947418213 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0347\n",
      "Epoch 40 Batch 100 Loss 0.0177\n",
      "Epoch 40 Batch 200 Loss 0.0295\n",
      "Epoch 40 Batch 300 Loss 0.0201\n",
      "Epoch 40 Loss 0.0330\n",
      "Time taken for 1 epoch 325.80824303627014 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0203\n",
      "Epoch 41 Batch 100 Loss 0.0153\n",
      "Epoch 41 Batch 200 Loss 0.0306\n",
      "Epoch 41 Batch 300 Loss 0.0202\n",
      "Epoch 41 Loss 0.0229\n",
      "Time taken for 1 epoch 325.17727613449097 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0160\n",
      "Epoch 42 Batch 100 Loss 0.0245\n",
      "Epoch 42 Batch 200 Loss 0.0144\n",
      "Epoch 42 Batch 300 Loss 0.0233\n",
      "Epoch 42 Loss 0.0194\n",
      "Time taken for 1 epoch 326.95297718048096 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0179\n",
      "Epoch 43 Batch 100 Loss 0.1082\n",
      "Epoch 43 Batch 200 Loss 1.3276\n",
      "Epoch 43 Batch 300 Loss 1.0708\n",
      "Epoch 43 Loss 0.5984\n",
      "Time taken for 1 epoch 324.8193140029907 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.9175\n",
      "Epoch 44 Batch 100 Loss 0.9274\n",
      "Epoch 44 Batch 200 Loss 0.8823\n",
      "Epoch 44 Batch 300 Loss 0.8742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Loss 0.8875\n",
      "Time taken for 1 epoch 325.23316740989685 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.9522\n",
      "Epoch 45 Batch 100 Loss 0.8048\n",
      "Epoch 45 Batch 200 Loss 0.8025\n",
      "Epoch 45 Batch 300 Loss 0.8563\n",
      "Epoch 45 Loss 0.8317\n",
      "Time taken for 1 epoch 324.5870668888092 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.8803\n",
      "Epoch 46 Batch 100 Loss 0.7334\n",
      "Epoch 46 Batch 200 Loss 0.7117\n",
      "Epoch 46 Batch 300 Loss 0.8107\n",
      "Epoch 46 Loss 0.7883\n",
      "Time taken for 1 epoch 325.42580819129944 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.8086\n",
      "Epoch 47 Batch 100 Loss 0.8653\n",
      "Epoch 47 Batch 200 Loss 0.6881\n",
      "Epoch 47 Batch 300 Loss 0.7401\n",
      "Epoch 47 Loss 0.7569\n",
      "Time taken for 1 epoch 324.8776595592499 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.6896\n",
      "Epoch 48 Batch 100 Loss 0.8408\n",
      "Epoch 48 Batch 200 Loss 0.6857\n",
      "Epoch 48 Batch 300 Loss 0.7674\n",
      "Epoch 48 Loss 0.7186\n",
      "Time taken for 1 epoch 329.4640769958496 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.6263\n",
      "Epoch 49 Batch 100 Loss 0.6887\n",
      "Epoch 49 Batch 200 Loss 0.7140\n",
      "Epoch 49 Batch 300 Loss 0.6762\n",
      "Epoch 49 Loss 0.6934\n",
      "Time taken for 1 epoch 324.9684913158417 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.7340\n",
      "Epoch 50 Batch 100 Loss 0.6724\n",
      "Epoch 50 Batch 200 Loss 0.7912\n",
      "Epoch 50 Batch 300 Loss 0.7395\n",
      "Epoch 50 Loss 0.6957\n",
      "Time taken for 1 epoch 327.0615816116333 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.7469\n",
      "Epoch 51 Batch 100 Loss 0.6082\n",
      "Epoch 51 Batch 200 Loss 0.5499\n",
      "Epoch 51 Batch 300 Loss 0.5580\n",
      "Epoch 51 Loss 0.6269\n",
      "Time taken for 1 epoch 324.9147136211395 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.5247\n",
      "Epoch 52 Batch 100 Loss 0.4965\n",
      "Epoch 52 Batch 200 Loss 0.4668\n",
      "Epoch 52 Batch 300 Loss 0.5248\n",
      "Epoch 52 Loss 0.5487\n",
      "Time taken for 1 epoch 326.5884282588959 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.4915\n",
      "Epoch 53 Batch 100 Loss 0.4680\n",
      "Epoch 53 Batch 200 Loss 0.3650\n",
      "Epoch 53 Batch 300 Loss 0.3916\n",
      "Epoch 53 Loss 0.4154\n",
      "Time taken for 1 epoch 324.9413869380951 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.2963\n",
      "Epoch 54 Batch 100 Loss 0.2687\n",
      "Epoch 54 Batch 200 Loss 0.3151\n",
      "Epoch 54 Batch 300 Loss 0.2748\n",
      "Epoch 54 Loss 0.3161\n",
      "Time taken for 1 epoch 328.14581656455994 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.2150\n",
      "Epoch 55 Batch 100 Loss 0.4322\n",
      "Epoch 55 Batch 200 Loss 0.2839\n",
      "Epoch 55 Batch 300 Loss 0.1802\n",
      "Epoch 55 Loss 0.2689\n",
      "Time taken for 1 epoch 325.23353338241577 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.2220\n",
      "Epoch 56 Batch 100 Loss 0.1591\n",
      "Epoch 56 Batch 200 Loss 0.1425\n",
      "Epoch 56 Batch 300 Loss 0.1856\n",
      "Epoch 56 Loss 0.3254\n",
      "Time taken for 1 epoch 325.863468170166 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.5751\n",
      "Epoch 57 Batch 100 Loss 0.3470\n",
      "Epoch 57 Batch 200 Loss 0.3071\n",
      "Epoch 57 Batch 300 Loss 0.2507\n",
      "Epoch 57 Loss 0.2940\n",
      "Time taken for 1 epoch 325.0697500705719 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.2372\n",
      "Epoch 58 Batch 100 Loss 0.1554\n",
      "Epoch 58 Batch 200 Loss 0.1403\n",
      "Epoch 58 Batch 300 Loss 0.1744\n",
      "Epoch 58 Loss 0.1721\n",
      "Time taken for 1 epoch 325.9628827571869 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.1448\n",
      "Epoch 59 Batch 100 Loss 0.1104\n",
      "Epoch 59 Batch 200 Loss 0.9464\n",
      "Epoch 59 Batch 300 Loss 0.5606\n",
      "Epoch 59 Loss 0.4589\n",
      "Time taken for 1 epoch 325.0710139274597 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.4064\n",
      "Epoch 60 Batch 100 Loss 0.2736\n",
      "Epoch 60 Batch 200 Loss 0.9095\n",
      "Epoch 60 Batch 300 Loss 0.7345\n",
      "Epoch 60 Loss 0.5238\n",
      "Time taken for 1 epoch 325.6370224952698 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.6134\n",
      "Epoch 61 Batch 100 Loss 0.3390\n",
      "Epoch 61 Batch 200 Loss 0.2889\n",
      "Epoch 61 Batch 300 Loss 0.3674\n",
      "Epoch 61 Loss 0.4185\n",
      "Time taken for 1 epoch 324.9724359512329 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.6999\n",
      "Epoch 62 Batch 100 Loss 0.9137\n",
      "Epoch 62 Batch 200 Loss 0.8964\n",
      "Epoch 62 Batch 300 Loss 0.9488\n",
      "Epoch 62 Loss 0.9458\n",
      "Time taken for 1 epoch 326.51081824302673 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.9249\n",
      "Epoch 63 Batch 100 Loss 0.9663\n",
      "Epoch 63 Batch 200 Loss 0.9354\n",
      "Epoch 63 Batch 300 Loss 0.9585\n",
      "Epoch 63 Loss 0.9341\n",
      "Time taken for 1 epoch 323.5024437904358 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.9471\n",
      "Epoch 64 Batch 100 Loss 0.9841\n",
      "Epoch 64 Batch 200 Loss 0.8951\n",
      "Epoch 64 Batch 300 Loss 0.8829\n",
      "Epoch 64 Loss 0.9197\n",
      "Time taken for 1 epoch 325.3458857536316 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 1.0071\n",
      "Epoch 65 Batch 100 Loss 0.9250\n",
      "Epoch 65 Batch 200 Loss 0.9171\n",
      "Epoch 65 Batch 300 Loss 0.9760\n",
      "Epoch 65 Loss 0.8946\n",
      "Time taken for 1 epoch 323.78282737731934 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.8539\n",
      "Epoch 66 Batch 100 Loss 0.7949\n",
      "Epoch 66 Batch 200 Loss 0.8378\n",
      "Epoch 66 Batch 300 Loss 0.8339\n",
      "Epoch 66 Loss 0.8733\n",
      "Time taken for 1 epoch 325.33812189102173 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.8719\n",
      "Epoch 67 Batch 100 Loss 0.7595\n",
      "Epoch 67 Batch 200 Loss 0.7130\n",
      "Epoch 67 Batch 300 Loss 0.6750\n",
      "Epoch 67 Loss 0.8576\n",
      "Time taken for 1 epoch 323.62844586372375 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.8029\n",
      "Epoch 68 Batch 100 Loss 0.7521\n",
      "Epoch 68 Batch 200 Loss 0.8435\n",
      "Epoch 68 Batch 300 Loss 0.8355\n",
      "Epoch 68 Loss 0.8440\n",
      "Time taken for 1 epoch 326.28043937683105 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.8646\n",
      "Epoch 69 Batch 100 Loss 0.8359\n",
      "Epoch 69 Batch 200 Loss 0.8948\n",
      "Epoch 69 Batch 300 Loss 0.8038\n",
      "Epoch 69 Loss 0.8290\n",
      "Time taken for 1 epoch 323.66809010505676 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.7704\n",
      "Epoch 70 Batch 100 Loss 0.7188\n",
      "Epoch 70 Batch 200 Loss 0.7392\n",
      "Epoch 70 Batch 300 Loss 0.8306\n",
      "Epoch 70 Loss 0.8169\n",
      "Time taken for 1 epoch 325.413592338562 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.8348\n",
      "Epoch 71 Batch 100 Loss 0.7676\n",
      "Epoch 71 Batch 200 Loss 0.7796\n",
      "Epoch 71 Batch 300 Loss 0.9279\n",
      "Epoch 71 Loss 0.8045\n",
      "Time taken for 1 epoch 323.6673288345337 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.8135\n",
      "Epoch 72 Batch 100 Loss 0.8121\n",
      "Epoch 72 Batch 200 Loss 0.8010\n",
      "Epoch 72 Batch 300 Loss 0.8316\n",
      "Epoch 72 Loss 0.7937\n",
      "Time taken for 1 epoch 325.54646372795105 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.8007\n",
      "Epoch 73 Batch 100 Loss 0.8364\n",
      "Epoch 73 Batch 200 Loss 0.7829\n",
      "Epoch 73 Batch 300 Loss 0.7489\n",
      "Epoch 73 Loss 0.7846\n",
      "Time taken for 1 epoch 323.77046275138855 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.7240\n",
      "Epoch 74 Batch 100 Loss 0.7418\n",
      "Epoch 74 Batch 200 Loss 0.7362\n",
      "Epoch 74 Batch 300 Loss 0.7378\n",
      "Epoch 74 Loss 0.7763\n",
      "Time taken for 1 epoch 325.39951181411743 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.8415\n",
      "Epoch 75 Batch 100 Loss 0.7334\n",
      "Epoch 75 Batch 200 Loss 0.7201\n",
      "Epoch 75 Batch 300 Loss 0.7197\n",
      "Epoch 75 Loss 0.7686\n",
      "Time taken for 1 epoch 323.7296531200409 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.8510\n",
      "Epoch 76 Batch 100 Loss 0.7353\n",
      "Epoch 76 Batch 200 Loss 0.7646\n",
      "Epoch 76 Batch 300 Loss 0.7798\n",
      "Epoch 76 Loss 0.7632\n",
      "Time taken for 1 epoch 328.5100336074829 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.8352\n",
      "Epoch 77 Batch 100 Loss 0.6985\n",
      "Epoch 77 Batch 200 Loss 0.8583\n",
      "Epoch 77 Batch 300 Loss 0.7379\n",
      "Epoch 77 Loss 0.7548\n",
      "Time taken for 1 epoch 323.83711433410645 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.7921\n",
      "Epoch 78 Batch 100 Loss 0.6700\n",
      "Epoch 78 Batch 200 Loss 0.7343\n",
      "Epoch 78 Batch 300 Loss 0.7864\n",
      "Epoch 78 Loss 0.7506\n",
      "Time taken for 1 epoch 324.54447841644287 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.7003\n",
      "Epoch 79 Batch 100 Loss 0.6834\n",
      "Epoch 79 Batch 200 Loss 0.6875\n",
      "Epoch 79 Batch 300 Loss 0.6956\n",
      "Epoch 79 Loss 0.7426\n",
      "Time taken for 1 epoch 323.89623165130615 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.7345\n",
      "Epoch 80 Batch 100 Loss 0.6228\n",
      "Epoch 80 Batch 200 Loss 0.7382\n",
      "Epoch 80 Batch 300 Loss 0.7369\n",
      "Epoch 80 Loss 0.7340\n",
      "Time taken for 1 epoch 326.16652631759644 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.7273\n",
      "Epoch 81 Batch 100 Loss 0.7094\n",
      "Epoch 81 Batch 200 Loss 0.6246\n",
      "Epoch 81 Batch 300 Loss 0.7050\n",
      "Epoch 81 Loss 0.7227\n",
      "Time taken for 1 epoch 323.8703887462616 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.7314\n",
      "Epoch 82 Batch 100 Loss 0.7280\n",
      "Epoch 82 Batch 200 Loss 0.7754\n",
      "Epoch 82 Batch 300 Loss 0.7040\n",
      "Epoch 82 Loss 0.7129\n",
      "Time taken for 1 epoch 325.74502515792847 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.7525\n",
      "Epoch 83 Batch 100 Loss 0.6025\n",
      "Epoch 83 Batch 200 Loss 0.7349\n",
      "Epoch 83 Batch 300 Loss 0.6520\n",
      "Epoch 83 Loss 0.7029\n",
      "Time taken for 1 epoch 323.8930115699768 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.7171\n",
      "Epoch 84 Batch 100 Loss 0.6294\n",
      "Epoch 84 Batch 200 Loss 0.5865\n",
      "Epoch 84 Batch 300 Loss 0.6413\n",
      "Epoch 84 Loss 0.6829\n",
      "Time taken for 1 epoch 325.6306862831116 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.7800\n",
      "Epoch 85 Batch 100 Loss 0.7582\n",
      "Epoch 85 Batch 200 Loss 0.7343\n",
      "Epoch 85 Batch 300 Loss 0.7067\n",
      "Epoch 85 Loss 0.7461\n",
      "Time taken for 1 epoch 323.99036049842834 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.6406\n",
      "Epoch 86 Batch 100 Loss 0.7363\n",
      "Epoch 86 Batch 200 Loss 0.6445\n",
      "Epoch 86 Batch 300 Loss 0.6979\n",
      "Epoch 86 Loss 0.6665\n",
      "Time taken for 1 epoch 325.80433917045593 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.6298\n",
      "Epoch 87 Batch 100 Loss 0.5917\n",
      "Epoch 87 Batch 200 Loss 0.4687\n",
      "Epoch 87 Batch 300 Loss 0.6119\n",
      "Epoch 87 Loss 0.5817\n",
      "Time taken for 1 epoch 324.0870966911316 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 0 Loss 0.4609\n",
      "Epoch 88 Batch 100 Loss 0.3399\n",
      "Epoch 88 Batch 200 Loss 0.2332\n",
      "Epoch 88 Batch 300 Loss 0.2525\n",
      "Epoch 88 Loss 0.2878\n",
      "Time taken for 1 epoch 325.7916042804718 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.1788\n",
      "Epoch 89 Batch 100 Loss 0.2646\n",
      "Epoch 89 Batch 200 Loss 0.1462\n",
      "Epoch 89 Batch 300 Loss 0.2173\n",
      "Epoch 89 Loss 0.1919\n",
      "Time taken for 1 epoch 324.13338017463684 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.6245\n",
      "Epoch 90 Batch 100 Loss 0.8625\n",
      "Epoch 90 Batch 200 Loss 0.7566\n",
      "Epoch 90 Batch 300 Loss 0.8574\n",
      "Epoch 90 Loss 0.8167\n",
      "Time taken for 1 epoch 326.8307077884674 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.6970\n",
      "Epoch 91 Batch 100 Loss 0.6602\n",
      "Epoch 91 Batch 200 Loss 0.7011\n",
      "Epoch 91 Batch 300 Loss 0.5776\n",
      "Epoch 91 Loss 0.6520\n",
      "Time taken for 1 epoch 324.4272599220276 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.4451\n",
      "Epoch 92 Batch 100 Loss 0.4050\n",
      "Epoch 92 Batch 200 Loss 1.0399\n",
      "Epoch 92 Batch 300 Loss 0.8769\n",
      "Epoch 92 Loss 0.6836\n",
      "Time taken for 1 epoch 325.11758613586426 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.7937\n",
      "Epoch 93 Batch 100 Loss 0.7725\n",
      "Epoch 93 Batch 200 Loss 0.7496\n",
      "Epoch 93 Batch 300 Loss 0.7632\n",
      "Epoch 93 Loss 0.7843\n",
      "Time taken for 1 epoch 324.45247054100037 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.7365\n",
      "Epoch 94 Batch 100 Loss 0.7363\n",
      "Epoch 94 Batch 200 Loss 0.7562\n",
      "Epoch 94 Batch 300 Loss 0.7005\n",
      "Epoch 94 Loss 0.7439\n",
      "Time taken for 1 epoch 325.1979396343231 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.6448\n",
      "Epoch 95 Batch 100 Loss 0.7691\n",
      "Epoch 95 Batch 200 Loss 0.7937\n",
      "Epoch 95 Batch 300 Loss 0.7000\n",
      "Epoch 95 Loss 0.7103\n",
      "Time taken for 1 epoch 324.576132774353 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.7398\n",
      "Epoch 96 Batch 100 Loss 0.6384\n",
      "Epoch 96 Batch 200 Loss 0.6507\n",
      "Epoch 96 Batch 300 Loss 0.5738\n",
      "Epoch 96 Loss 0.6325\n",
      "Time taken for 1 epoch 325.38930106163025 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.5851\n",
      "Epoch 97 Batch 100 Loss 0.5327\n",
      "Epoch 97 Batch 200 Loss 0.5875\n",
      "Epoch 97 Batch 300 Loss 0.8810\n",
      "Epoch 97 Loss 0.6665\n",
      "Time taken for 1 epoch 324.54405331611633 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.6454\n",
      "Epoch 98 Batch 100 Loss 0.4847\n",
      "Epoch 98 Batch 200 Loss 0.4505\n",
      "Epoch 98 Batch 300 Loss 0.3124\n",
      "Epoch 98 Loss 0.4319\n",
      "Time taken for 1 epoch 326.2825891971588 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.2341\n",
      "Epoch 99 Batch 100 Loss 0.1998\n",
      "Epoch 99 Batch 200 Loss 0.3991\n",
      "Epoch 99 Batch 300 Loss 0.2383\n",
      "Epoch 99 Loss 0.3148\n",
      "Time taken for 1 epoch 324.6762475967407 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.1728\n",
      "Epoch 100 Batch 100 Loss 0.1641\n",
      "Epoch 100 Batch 200 Loss 0.2389\n",
      "Epoch 100 Batch 300 Loss 0.3713\n",
      "Epoch 100 Loss 0.2727\n",
      "Time taken for 1 epoch 327.3695831298828 sec\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.1950\n",
      "Epoch 101 Batch 100 Loss 0.2353\n",
      "Epoch 101 Batch 200 Loss 0.2674\n",
      "Epoch 101 Batch 300 Loss 0.1623\n",
      "Epoch 101 Loss 0.2230\n",
      "Time taken for 1 epoch 325.14376997947693 sec\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.1176\n",
      "Epoch 102 Batch 100 Loss 0.1784\n",
      "Epoch 102 Batch 200 Loss 0.0967\n",
      "Epoch 102 Batch 300 Loss 0.3569\n",
      "Epoch 102 Loss 0.2970\n",
      "Time taken for 1 epoch 326.8045303821564 sec\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.7634\n",
      "Epoch 103 Batch 100 Loss 0.5853\n",
      "Epoch 103 Batch 200 Loss 0.3165\n",
      "Epoch 103 Batch 300 Loss 0.2153\n",
      "Epoch 103 Loss 0.3849\n",
      "Time taken for 1 epoch 325.09054160118103 sec\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.1263\n",
      "Epoch 104 Batch 100 Loss 0.2234\n",
      "Epoch 104 Batch 200 Loss 0.1619\n",
      "Epoch 104 Batch 300 Loss 0.1570\n",
      "Epoch 104 Loss 0.1446\n",
      "Time taken for 1 epoch 327.7081310749054 sec\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.1463\n",
      "Epoch 105 Batch 100 Loss 0.0780\n",
      "Epoch 105 Batch 200 Loss 0.9269\n",
      "Epoch 105 Batch 300 Loss 0.4103\n",
      "Epoch 105 Loss 0.3393\n",
      "Time taken for 1 epoch 325.103102684021 sec\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.2926\n",
      "Epoch 106 Batch 100 Loss 0.1661\n",
      "Epoch 106 Batch 200 Loss 0.1519\n",
      "Epoch 106 Batch 300 Loss 0.1919\n",
      "Epoch 106 Loss 0.2069\n",
      "Time taken for 1 epoch 327.8233554363251 sec\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.2170\n",
      "Epoch 107 Batch 100 Loss 0.3454\n",
      "Epoch 107 Batch 200 Loss 0.1197\n",
      "Epoch 107 Batch 300 Loss 0.1212\n",
      "Epoch 107 Loss 0.1446\n",
      "Time taken for 1 epoch 325.102756023407 sec\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.0896\n",
      "Epoch 108 Batch 100 Loss 0.0980\n",
      "Epoch 108 Batch 200 Loss 0.2529\n",
      "Epoch 108 Batch 300 Loss 0.2088\n",
      "Epoch 108 Loss 0.1742\n",
      "Time taken for 1 epoch 325.89587664604187 sec\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.1370\n",
      "Epoch 109 Batch 100 Loss 0.1270\n",
      "Epoch 109 Batch 200 Loss 0.0905\n",
      "Epoch 109 Batch 300 Loss 0.4735\n",
      "Epoch 109 Loss 0.1322\n",
      "Time taken for 1 epoch 325.1078531742096 sec\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.1588\n",
      "Epoch 110 Batch 100 Loss 0.1390\n",
      "Epoch 110 Batch 200 Loss 0.1573\n",
      "Epoch 110 Batch 300 Loss 0.1973\n",
      "Epoch 110 Loss 0.1734\n",
      "Time taken for 1 epoch 326.88222193717957 sec\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.2078\n",
      "Epoch 111 Batch 100 Loss 0.1392\n",
      "Epoch 111 Batch 200 Loss 0.1804\n",
      "Epoch 111 Batch 300 Loss 0.3311\n",
      "Epoch 111 Loss 0.2845\n",
      "Time taken for 1 epoch 325.1403875350952 sec\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.1345\n",
      "Epoch 112 Batch 100 Loss 0.1540\n",
      "Epoch 112 Batch 200 Loss 0.4093\n",
      "Epoch 112 Batch 300 Loss 0.2206\n",
      "Epoch 112 Loss 0.2183\n",
      "Time taken for 1 epoch 325.843398809433 sec\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.1696\n",
      "Epoch 113 Batch 100 Loss 0.1683\n",
      "Epoch 113 Batch 200 Loss 0.1059\n",
      "Epoch 113 Batch 300 Loss 0.1514\n",
      "Epoch 113 Loss 0.1584\n",
      "Time taken for 1 epoch 328.38173508644104 sec\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.1141\n",
      "Epoch 114 Batch 100 Loss 0.7424\n",
      "Epoch 114 Batch 200 Loss 0.3302\n",
      "Epoch 114 Batch 300 Loss 0.1909\n",
      "Epoch 114 Loss 0.3377\n",
      "Time taken for 1 epoch 327.74437284469604 sec\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.1789\n",
      "Epoch 115 Batch 100 Loss 0.1753\n",
      "Epoch 115 Batch 200 Loss 0.7203\n",
      "Epoch 115 Batch 300 Loss 0.3144\n",
      "Epoch 115 Loss 0.3359\n",
      "Time taken for 1 epoch 325.11401748657227 sec\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.1924\n",
      "Epoch 116 Batch 100 Loss 0.1741\n",
      "Epoch 116 Batch 200 Loss 0.1732\n",
      "Epoch 116 Batch 300 Loss 0.0911\n",
      "Epoch 116 Loss 0.1544\n",
      "Time taken for 1 epoch 326.83757638931274 sec\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.1339\n",
      "Epoch 117 Batch 100 Loss 0.1923\n",
      "Epoch 117 Batch 200 Loss 0.3487\n",
      "Epoch 117 Batch 300 Loss 0.4183\n",
      "Epoch 117 Loss 0.4357\n",
      "Time taken for 1 epoch 324.900066614151 sec\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.9467\n",
      "Epoch 118 Batch 100 Loss 0.2513\n",
      "Epoch 118 Batch 200 Loss 0.2031\n",
      "Epoch 118 Batch 300 Loss 0.2969\n",
      "Epoch 118 Loss 0.3340\n",
      "Time taken for 1 epoch 326.66609382629395 sec\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.2510\n",
      "Epoch 119 Batch 100 Loss 0.2882\n",
      "Epoch 119 Batch 200 Loss 0.2285\n",
      "Epoch 119 Batch 300 Loss 0.1772\n",
      "Epoch 119 Loss 0.2780\n",
      "Time taken for 1 epoch 325.0303659439087 sec\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.1223\n",
      "Epoch 120 Batch 100 Loss 0.1111\n",
      "Epoch 120 Batch 200 Loss 0.0942\n",
      "Epoch 120 Batch 300 Loss 0.1895\n",
      "Epoch 120 Loss 0.1433\n",
      "Time taken for 1 epoch 326.69643354415894 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 120\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    \n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['\\x00']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id]\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '\\x01':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result), :len(sentence)]\n",
    "  plot_attention(attention_plot, [c for c in sentence], [c for c in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-60'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAM0FDomq3E"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fr1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-0265fdc9d961>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfr1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"correct => \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfr1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fr1' is not defined"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "i = randint(0,len(fr1)-1)\n",
    "\n",
    "print(\"correct => \"+fr2[i])\n",
    "translate(fr1[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSx2iM36EZQZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \u0000esta es mi vida.\u0001\n",
      "Predicted translation: esta es mi vida .\u0001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-00c5d5f4c50a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'esta es mi vida.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-afa32bc7bbb7>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mattention_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m   \u001b[0mplot_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_plot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-3bda2de60e70>\u001b[0m in \u001b[0;36mplot_attention\u001b[1;34m(attention, senence, predicted_sentence)\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mfontdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'fontsize'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m   \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m   \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpredicted_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3LLCx3ZE0Ls"
   },
   "outputs": [],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUQVLVqUE1YW"
   },
   "outputs": [],
   "source": [
    "# wrong translation\n",
    "translate(u'trata de averiguarlo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTe5P5ioMJwN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
